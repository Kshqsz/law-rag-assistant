# coding: utf-8
"""
æ–‡æ¡£åˆå¹¶ä¸æ ¼å¼åŒ–æ¨¡å—

åŠŸèƒ½è¯´æ˜ï¼š
- combine_law_docs(docs): å°†æ¥è‡ªå‘é‡åº“çš„æ³•å¾‹æ–‡æ¡£æŒ‰ä¹¦ç±åˆ†ç»„ï¼Œæ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
  æŒ‰ç…§"ç›¸å…³æ³•å¾‹ï¼šã€Šä¹¦åã€‹"çš„æ ¼å¼ç»„ç»‡æ³•å¾‹æ¡æ–‡ï¼Œä¾¿äºä¼ é€’ç»™ LLM
  
- combine_web_docs(docs): å°†æ¥è‡ªç½‘é¡µçš„æ–‡æ¡£æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
  æŒ‰ç…§"ç›¸å…³ç½‘é¡µï¼šç½‘é¡µæ ‡é¢˜"ã€"ç½‘é¡µåœ°å€ï¼šURL"çš„æ ¼å¼ç»„ç»‡ç½‘é¡µå†…å®¹

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from langchain.docstore.document import Document
    from law_ai.combine import combine_law_docs, combine_web_docs
    
    # åˆ›å»ºç¤ºä¾‹æ³•å¾‹æ–‡æ¡£
    law_docs = [
        Document(
            page_content="ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äººçš„ï¼Œå¤„æ­»åˆ‘ã€‚",
            metadata={"book": "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•"}
        ),
        Document(
            page_content="ç¬¬äºŒç™¾ä¸‰åä¸‰æ¡ è¿‡å¤±è‡´äººæ­»äº¡çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘",
            metadata={"book": "ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•"}
        )
    ]
    
    # åˆå¹¶æ³•å¾‹æ–‡æ¡£
    law_text = combine_law_docs(law_docs)
    print(law_text)
    # è¾“å‡º:
    # ç›¸å…³æ³•å¾‹ï¼šã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹
    # ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äººçš„ï¼Œå¤„æ­»åˆ‘ã€‚
    # ç¬¬äºŒç™¾ä¸‰åä¸‰æ¡ è¿‡å¤±è‡´äººæ­»äº¡çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘
    
    # åˆ›å»ºç¤ºä¾‹ç½‘é¡µæ–‡æ¡£
    web_docs = [
        Document(
            page_content="æ ¹æ®æœ€æ–°æ³•å¾‹è§„å®šï¼Œæ•…æ„æ€äººéœ€è¦æ‰¿æ‹…åˆ‘äº‹è´£ä»»ã€‚",
            metadata={"title": "åˆ‘æ³•è§£é‡Š", "link": "https://example.com/law"}
        )
    ]
    
    # åˆå¹¶ç½‘é¡µæ–‡æ¡£
    web_text = combine_web_docs(web_docs)
    print(web_text)
    # è¾“å‡º:
    # ç›¸å…³ç½‘é¡µï¼šåˆ‘æ³•è§£é‡Š
    # ç½‘é¡µåœ°å€ï¼šhttps://example.com/law
    # æ ¹æ®æœ€æ–°æ³•å¾‹è§„å®šï¼Œæ•…æ„æ€äººéœ€è¦æ‰¿æ‹…åˆ‘äº‹è´£ä»»ã€‚
"""
from typing import List
from collections import defaultdict

from langchain.docstore.document import Document

# example:
#   ç›¸å…³æ³•å¾‹ï¼šã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹
#   ç¬¬äºŒç™¾ä¸‰åäºŒæ¡ æ•…æ„æ€äººçš„ï¼Œå¤„æ­»åˆ‘ã€‚
#   ç¬¬äºŒç™¾ä¸‰åä¸‰æ¡ è¿‡å¤±è‡´äººæ­»äº¡çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šä¸ƒå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘
def combine_law_docs(docs: List[Document]) -> str:
    law_books = defaultdict(list)
    for doc in docs:
        metadata = doc.metadata
        if 'book' in metadata:
            content = doc.page_content.strip()
            # åªè¿‡æ»¤ç©ºå†…å®¹å’Œçº¯æ ‡é¢˜ï¼ˆå¦‚"ç¬¬ä¸€èŠ‚ ä¸€èˆ¬è§„å®š"è¿™ç§æ²¡æœ‰å®é™…æ¡æ–‡çš„ï¼‰
            # çº¯æ ‡é¢˜ç‰¹å¾ï¼šä»¥"ç¬¬"å¼€å¤´ï¼Œä¸”ä¸åŒ…å«"æ¡"å­—
            is_empty_title = (content.startswith('ç¬¬') and 
                             'æ¡' not in content and 
                             len(content) < 30)
            
            # ä¿ç•™æ‰€æœ‰éç©ºä¸”éçº¯æ ‡é¢˜çš„æ–‡æ¡£ï¼ˆåŒ…æ‹¬ç”¨æˆ·ä¸Šä¼ çš„çŸ­æ–‡æ¡£ï¼‰
            if content and not is_empty_title:
                law_books[metadata["book"]].append(doc)

    law_str = ""
    book_num = 0
    for book, docs in law_books.items():
        if docs:  # ç¡®ä¿æœ‰æœ‰æ•ˆæ–‡æ¡£
            book_num += 1
            law_str += f"### ğŸ“– {book_num}. ã€Š{book}ã€‹\n\n"
            for i, doc in enumerate(docs, 1):
                content = doc.page_content.strip("\n")
                # ä¸ºæ¯ä¸ªæ¡æ–‡æ·»åŠ åºå·å’Œç¼©è¿›
                law_str += f"**[{i}]** {content}\n\n"
            law_str += "---\n\n"

    return law_str.rstrip("---\n\n") if law_str else ""


def combine_web_docs(docs: List[Document]) -> str:
    # æ³•å¾‹ç›¸å…³å…³é”®è¯
    law_keywords = ['æ³•', 'åˆ‘', 'æ¡', 'è§„å®š', 'æ³•å¾‹', 'æ¡ä¾‹', 'åˆ‘æ³•', 'æ°‘æ³•', 
                     'è¯‰è®¼', 'æƒåˆ©', 'è´£ä»»', 'åˆ¤', 'ç½š', 'èµ”å¿', 'åˆ‘æœŸ',
                     'çŠ¯ç½ª', 'è¿æ³•', 'è¯‰è®¼', 'ä»²è£', 'æ³•é™¢', 'æ£€å¯Ÿ',
                     'å¾‹å¸ˆ', 'å…¬å®‰', 'å¸æ³•', 'æ³•åˆ¶', 'å®ªæ³•']
    
    web_str = ""
    valid_docs = []
    
    for doc in docs:
        title = doc.metadata.get('title', '')
        content = doc.page_content.strip("\n")
        
        # æ£€æŸ¥æ ‡é¢˜æˆ–å†…å®¹æ˜¯å¦åŒ…å«æ³•å¾‹å…³é”®è¯
        is_law_related = any(keyword in title or keyword in content for keyword in law_keywords)
        
        # è¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„å…³é”®è¯
        irrelevant_keywords = ['Python', 'CSDN', 'ç¼–ç¨‹', 'ä»£ç ', 'ç½‘ç»œä¿¡æ¯å®‰å…¨', 'è¯¾åä¹ é¢˜', 
                               'ç™¾åº¦æ–‡åº“', 'Wordæ–‡æ¡£', 'æ‰¹é‡è¯»å–', 'æ•°æ®åº“', 'API']
        is_irrelevant = any(keyword in title or keyword in content[:200] for keyword in irrelevant_keywords)
        
        if is_law_related and not is_irrelevant:
            valid_docs.append(doc)
    
    # æ ¼å¼åŒ–è¾“å‡º
    for i, doc in enumerate(valid_docs, 1):
        title = doc.metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
        link = doc.metadata.get('link', '')
        content = doc.page_content.strip("\n")
        
        web_str += f"### ğŸŒ {i}. {title}\n\n"
        if link:
            web_str += f"**æ¥æºï¼š** [{link}]({link})\n\n"
        web_str += f"{content}\n\n"
        
        if i < len(valid_docs):  # ä¸æ˜¯æœ€åä¸€ä¸ªæ‰åŠ åˆ†éš”çº¿
            web_str += "---\n\n"

    return web_str
